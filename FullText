{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chapter 2: Statistical Learning","metadata":{}},{"cell_type":"markdown","source":"# What is Statistical Learning?\n\n## The relationship\nThe relationship between dependent and independent variables can be written as:\n\nY = f(X) + e\n\nImagine X as matrix or a dataframe with all the vectors/columns of pridictor variables. Imagine Y as a vector/column. \n\nWe predict Y using \n\nY' = f'(X)\n\nWhere Y' is the resulting prediction of Y. \nf' is our estimator of f (actual relationship)\n\nThe accuracy of Y' as a prediction for Y depends on two quantities,\nwhich we will call the reducible error and the irreducible error. \n\nthe **reducible error** comes from f'. We can reduce the error by using most appropreate statistical learning technique. \n\nEven if we use the perfect fuction for f, we still end up with the **irreducible error** which comes from the other independent variables which are not included in the model. It is important to keep in mind that the\nirreducible error will always provide an upper bound on the accuracy of\nour prediction for Y . This bound is almost always unknown in practice.\n\n## Inference and prediction\nThere are two types of problem we have, inference and prediction.\n\nWhen our goal is understand the association between the Y and X rather than the accuracy Y, that is called inference problem and vice versa is prediction problem. \n\nPrediction: When the aim is to accurately predict the response for 'future'/'OOS' observations.\nInference: When the aim is to understand the relationship between the response and the predictors using 'given sample'.\n\nDepending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating f may be appropriate.\nFor example, linear models allow for relatively simple and interpretable inference, but may not yield as accurate predictions as some other approaches. In contrast, some of the highly non-linear approaches that we discuss in the later chapters of this book can potentially provide quite accurate predictions for Y , but this comes at the expense of a less\ninterpretable model for which inference is more challenging. \n\n## Estimating f\nOur goal is to apply a statistical learning method to the training data in order to estimate the unknown function f.\n\nmost statistical learning methods for this task can be characterized as either parametric or non-parametric.\n\n### Parametric methods\nParametric methods involve a two-step model-based approach.\n\nFirst, we make an assumption about the functional form, or shape,of f, such as linear form.\nAfter a model has been selected, we need a procedure that uses the training data to fit or train the model, such as Orinary Least Squares.\n\nIf the chosen model is too far from the true f, then our estimate will be poor. We can try to address this problem by choosing flexible models that can fit many different possible functional forms for f. But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a\nphenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely.\nOver-fitting is an undesirable situation because the fit obtained will not yield accurate estimates of the response on new\nobservations that were not part of the original training data set.\n\n### Non-Parametric Methods\nNon-parametric methods do not make explicit assumptions about the functional form of f. Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly.\n\nBy avoiding the assumption of a particular functional form for f, they have the potential to accurately fit a wider range of possible shapes for f.\n\nBut non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating f to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for f.\n\n## The Trade-Off Between Prediction Accuracy and Model Interpretability\nIf we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between Y and X1,X2, . . . ,Xp. In contrast, very flexible approaches, such as the (non-parametric) splines and the boosting methods can\nlead to such complicated estimates of f that it is difficult to understand how any individual predictor is associated with the response.\n\nWe have established that when inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. In some settings, however, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest. For instance, if we seek to develop an algorithm to predict the price of a\nstock, our sole requirement for the algorithm is that it predict accurately - interpretability is not a concern. In this setting, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods.\n\n## Supervised Versus Unsupervised Learning\nSupervised learning problems will have a response variable to supervise our analysis. Whereas, no such resonse variable is available in unsupervised learning problems. Cluster analysis most common unsupervised leaning technique in which several distinguishable groups are identified based on clusters of observations of several valriables. \n\n## Regression Versus Classification Problems\nWe tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems.\nDespite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well.","metadata":{}},{"cell_type":"markdown","source":"# Assessing Model Accuracy\nModel selection is most challenging part of the model buildng. We chose among all the studied methods based on cerain criteria such as model performance or accuracy. \n\n## Measuring the Quality of Fit\nTo measure how close the prediction are to the actuals on given data set on which the model is trained, we have sevaral metrics to measure. \n\nMSE (Mean Squared Error) is mean of squares of differences of actuals and predictions. Think about MSE as variance. \n\nMSE = ${\\Sigma(y_i - f'(x_i))} \\over {n}$\n\nThis is called training/in-sample MSE. in general, we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the pre-dictions that we obtain when we apply our method to previously unseen test data. So test/OOS MSE is what we really care about.\n\nIn most cases, traning MSE decreases with incease in model fexibility. However, test MSE decreases initially upto certain extent of flexibility but increaes after a thrshold is reached. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE.\n\nWhen test data is not available, we use cross-validation, which is a method for estimating the test MSE using the training data.\n\n## The Bias-Variance Trade-Off\nIn order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves\nlow variance and low bias. \n\nVariance refers to the amount by which f' would change if we estimated it using a different training data set. In general, more flexible statistical methods have higher variance.\n\nbias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. Generally, more flexible methods result in less bias.\nFor example, linear regression assumes that there is a linear relationship between Y and X1,X2, . . . ,Xp. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear\nregression will undoubtedly result in some bias in the estimate of f.\n\nAs a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases.\n\nThe relationship between bias, variance, and test set MSE is referred to as the bias-variance trade-off. \nGood test set performance of a statistical learning method requires low variance as well as low squared bias. This is referred to as a trade-off because it is easy to obtain a method with extremely low bias but high variance (for instance, by drawing a curve that passes through every single training observation) or a method with very low variance but high\nbias (by fitting a horizontal line to the data). The challenge lies in finding a method for which both the variance and the squared bias are low.\n\nIn a real-life situation in which f is unobserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method. Nevertheless, one should always keep the bias-variance trade-off in mind.\n\n## The Classification Setting\nmany of the concepts that we have encountered, such as the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that yi is no longer quantitative.\n\nIn classification problem, since the response variable is qualitative, we use 'error rate' to measure the model accuracy. \nThe error rate is defined as the ratio of total mis-classified observations to the total observations (Confusion matrix is one way to show these).","metadata":{}},{"cell_type":"markdown","source":"# Chapter3: Linear Regression","metadata":{}},{"cell_type":"markdown","source":"# Simple linear regression\n\ny' = b0' + b1' x\n\nwhere y' is the predicted value of actual y, b0' and b1' are estimated values of true b0 and b1.\n\ny = y' + e\n\nwhere e is the residual or error\n\ny = b0' + b1' x + e\n\n## Estimating the Coefficients\n\nRSS (Residual Sum of Squares) = e1^2 + e2^2 ... en^2\n\nwhere e = y - y' or ei = yi - yi'\n\nRSS = (y1 - b0' - b1' x1)^2 + (y2 - b0' - b1' x2)^2 + .... (yn - b0' - b1' xn)^2\n\nIn least squares method, the objective function is to find the b0' and b1' such that RSS is minimum. \n\ny~ = sample mean of y\n\nx~ = sample mean of x\n\n## Assessing the Accuracy of the Coefficient Estimates\nThe analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of bias. If we use the sample mean μ' to estimate μ, this estimate is unbiased, in the sense that on average, we expect μ' to equal μ.\n\nif we could average a huge number of estimates of μ obtained from a huge number of sets of observations, then this average\nwould exactly equal μ. Hence, an unbiased estimator does not systematically over- or under-estimate the true parameter.\n\nhow accurate is the sample mean μ' as an estimate of μ? We have established that the average of μ'’s over many data sets will be very close to μ, but that a single estimate μ' may be a substantial underestimate or overestimate of μ.\nHow far off will that single estimate of μ' be? In general, we answer this question by computing the standard error of μ', written as SE(μ'). We have well-known formula\n\nVar(μ') = SE(μ')^2 = segma^2/n\n\nwhere segma is the standard deviation of each of the realizations yi of Y. Roughly speaking, the standard error tells us the average amount that this estimate μ' differs from the actual value of μ. Above Equation also tells us how this deviation shrinks with n—the more observations we have, the smaller the standard error of μ'. In a similar vein, we can wonder how close b0' and b1' are to the true values b0 and b1.\n\nFor the SE formulas to be strictly valid, we need to assume that the errors ei for each observation have common variance segma^2 and are uncorrelated. This is generally not true, but the formula still turns out to be a good approximation. Notice in the formula that SE(b1') is smaller when the xi are more spread out; intuitively we have more leverage to estimate a slope when this is the case. We also see that SE(b01) would be the same as SE(μ') if x~ were zero (in which case b0' would be equal to y~). \nIn general, segma^2 is not known, but can be estimated from the data. This estimate of  is known as the residual standard error, and is given by the formula \n\nRSE = $\\sqrt {RSS/(n-2)}$\n\nStandard errors can be used to compute confidence intervals. A 95 % confidence confidence interval is defined as a range of values such that with 95% interval probability, the range will contain the true unknown value of the parameter.\nThe range is defined in terms of lower and upper limits computed from the sample of data. A 95% confidence interval has the following property:\nif we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter. That is, there is approximately a 95 % chance that the interval will contain the true value of b1.\n\nExample: In the case of the advertising data, the 95 % confidence interval for b0 is [6.130, 7.935] and the 95 % confidence interval for b1 is [0.042, 0.053]. Therefore, we can conclude that in the absence of any advertising, sales will, on average, fall somewhere between 6,130 and 7,935 units. Furthermore, for each 1,000 dollar increase in television advertising, there will be an average increase in sales of between 42 and 53 units.\n\nAs we can see, b0 same as 'mean' of y when x=0. whereas b1 is 'change' in y with unit change in x. \n\nStandard errors can also be used to perform hypothesis tests on coefficients. The most common hypothesis test involves testing the null hypothesis of \n\nH0 : There is no relationship between X and Y: b1=0\nHa : There is some relationship between X and Y : b1 != 0\n\nTo test the null hypothesis, we need to determine whether b1', our estimate for b1, is sufficiently far from zero that we can be confident that b1 is non-zero. How far is far enough? This of course depends on the accuracy of b1' — that is, it depends on SE(b1'). If SE(b1') is small, then even relatively small values of b1' may provide strong evidence that b1 != 0, and hence that there is a relationship between X and Y . In contrast, if SE(b1') is large, then b1' must be large in absolute value in order for us to reject the null hypothesis. In practice, we compute a t-statistic\n\nt = (b1'-0)/SE(b1')\n\nwhich measures the number of standard deviations that b1' is away from 0.\n\nthe probability of observing any number equal to |t| or larger in absolute value, assuming b1 = 0. We call this probability\nthe p-value. Therefore, if p-value is sufficiently small, we reject the null hypothesis and declare that there is association between Y and X.\n\n||Coefficient| Std. error| t-statistic| p-value|\n|-|-|-|-|-|\n|Intercept| 7.0325| 0.4578| 15.36| < 0.0001|\n|TV| 0.0475| 0.0027| 17.67| < 0.0001|\n\nAbove table provides details of the least squares model for the regression of number of units sold on TV advertising budget for the Advertising data.\nNotice that the coefficients for b0' and b1' are very large relative to their standard errors, so the t-statistics are also large; the probabilities of seeing such values if H0 is true are virtually zero. Hence we can conclude that \nb0 != 0 and b1 != 0\n\n## Assessing the Accuracy of the Model\nOnce we have rejected the null hypothesis in favor of the alternative hypothesis, it is natural to want to quantify the extent to which the model fits the data. The quality of a linear regression fit is typically assessed using two related quantities: \nthe residual standard error (RSE) and \nthe R2 statistic.\n\n### Residual Standard Error\nDue to the presence of error terms, even if we knew the true regression line (i.e. even if b0 and b1 were known), we would not be able to perfectly predict Y from X. \nThe RSE is an estimate of the standard deviation of e. Roughly speaking, it is the average amount that the response\nwill deviate from the true regression line.\n\nRSE = $\\sqrt {RSS \\over (n-2)}$\n\n = $\\sqrt {\\sum(yi - yi')^2 \\over (n-2)}$\n \nIn the case of the advertising data, we see from the linear regression output that the RSE is 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average. \nAnother way to think about this is that even if the model were correct and the true values of the unknown coefficients b0\nand b1 were known exactly, any prediction of sales on the basis of TV advertising would still be off by about 3,260 units on average. Of course, whether or not 3,260 units is an acceptable prediction error depends on the problem context. In the advertising data set, the mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23 %. \n\nIf predictions are closure to actuals, then RSE is lesser, which means the model fits data well.\n\n## R^2 Statistic\nThe RSE provides an absolute measure of lack of fit of the model to the data. But since it is measured in the units of Y , it is not always clear what constitutes a good RSE. The R2 statistic provides an alternative measure of fit. It takes the form of a proportion—the proportion of variance explained—and so it always takes on a value between 0 and 1, and is independent of the scale of Y.\n\nR2 = (TSS − RSS) / TSS\n = 1− (RSS/TSS)\n \nwhere TSS (total Sum of Squares) = $\\sum(y_i - \\bar y)^2 $\n\nTSS measures the total variance in the response Y , and can be thought of as the amount of variability inherent in the response before the regression is performed. \nIn contrast, RSS measures the amount of variability that is left unexplained after performing the regression. \nHence, TSS−RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and R2 measures the proportion of variability in Y that can be explained using X.\n\nA number near 0 indicates that the regression does not explain much of the variability in the response; this might occur\nbecause the linear model is wrong, or the error variance segma^2 is high, or both.\n\nThe R2 statistic has an interpretational advantage over the RSE, since unlike the RSE, it always lies between 0 and 1. However, it can still be challenging to determine what is a good R2 value, and in general, this will depend on the application. For instance, in certain problems in physics, we may know that the data truly comes from a linear model with\na small residual error. In this case, we would expect to see an R2 value that is extremely close to 1, and a substantially smaller R2 value might indicate a serious problem with the experiment in which the data were generated. On the other hand, in typical applications in biology, psychology, marketing, and other domains, the linear model (3.5) is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large. In this setting, we would expect only a very small proportion of the variance in the response to be explained by the\npredictor, and an R2 value well below 0.1 might be more realistic.\n\nThe R2 statistic is a measure of the linear relationship between X and Y. Whereas, also a measure of the linear relationship between X and Y. In simple linear regression setting, R2 = r2. In other words, the squared correlation\nand the R2 statistic are identical. However, in multiple linear regression problem, in which we use several predictors simultaneously to predict the response. The concept of correlation between the predictors and the response does not extend automatically to this setting, since correlation quantifies the association between a single pair of variables rather than between a larger number of variables. We will see that R2 fills this role.","metadata":{}},{"cell_type":"markdown","source":"# Multiple Linear Regression","metadata":{}}]}